{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires: NLTK, Gensim and Networkx\n",
    "\n",
    "Dataset Used: DBLP Citation Network V6 (http://arnetminer.org/citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the whole system from ground up use the following code.\n",
    "\n",
    "This however will take at least 6 hours and use up over 6GB of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from citemachine.corpus.dblp import DBLP\n",
    "from citemachine import topic_model\n",
    "from citemachine.text_process import CorpusPreprocessor\n",
    "from citemachine.evaluation import precision, recall\n",
    "from citemachine.graph import CommunityRank, adj_lists_to_directed_graph\n",
    "from citemachine.recommender import LDARecommender, CiteMachine\n",
    "\n",
    "#set to the location where dblp is stored\n",
    "path_to_dblp = 'dblp.txt'\n",
    "#set to a small value like 5000 to test on a subset of the data\n",
    "num_docs = 2000\n",
    "\n",
    "#parses the dataset\n",
    "dblp = DBLP(path_to_dblp, max_docs=num_docs)\n",
    "\n",
    "#preprocesses the data and trains an LDA\n",
    "recommender = LDARecommender(corpus=dblp, num_topics=100, train_at_init=True)\n",
    "\n",
    "#builds a reference graph then finds communities and ranks documents in each one using PageRank\n",
    "references_graph = adj_lists_to_directed_graph(dblp.references)\n",
    "comrank = CommunityRank(references_graph)\n",
    "\n",
    "# Combines the LDA model with the community graphs to create the final recommendation system\n",
    "citem = CiteMachine(recommender, comrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#citemachine evaluation code.\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, LabeledSentence\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "papers = pd.read_csv('data-kw-216432.csv')\n",
    "\n",
    "random.seed(1)\n",
    "#select 2000 documens randomly\n",
    "ids_sel = random.sample(list(papers['INDEX']), 2000)\n",
    "\n",
    "#define function to compare citation\n",
    "def compare_citations(ID):        \n",
    "    #TODO: evaluation of this   \n",
    "    #ars: take the id as integer\n",
    "    #returns the number of citations in common between sugested and actual\n",
    "\n",
    "    abstracts_fromdb = list(papers[papers['INDEX'] == ID]['ABSTRACT'])\n",
    "    citations_fromdb = list(papers[papers['INDEX'] == ID]['CITATIONS'])\n",
    "    #print(abstracts_fromdb)\n",
    "    pred_citations =[]\n",
    "    for pair in citem.get_recommended_docs_for_text(abstracts_fromdb[0]):\n",
    "        ID2, score = pair \n",
    "        pred_citations.append(ID2)\n",
    "    print('pred',pred_citations)    \n",
    "    for i in citations_fromdb:\n",
    "        actual_citations = i.split(\";\")\n",
    "    actual_citations = list(map(int, actual_citations))\n",
    "    #print('actual',actual_citations) \n",
    "    if len(set(pred_citations) & set(actual_citations))>0:\n",
    "        #print(ID,len(set(pred_citations) & set(actual_citations)))\n",
    "    acc = len(set(pred_citations) & set(actual_citations))/(min(len(pred_citations),len(actual_citations)))\n",
    "    return acc\n",
    "    \n",
    "# calculates the accuracy.    \n",
    "#accuracy = {}    \n",
    "\n",
    "for ns in ids_sel:\n",
    "    #print(compare_citations(int(ns)))\n",
    "    accuracy [ns] = compare_citations(int(ns))\n",
    "#print('ACCURACY=',sum(accuracy[ns] for ns in accuracy) / len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30045 1\n",
      "385564 1\n",
      "1117826 1\n",
      "606014 1\n",
      "593853 1\n",
      "233202 1\n",
      "1062285 1\n",
      "1111729 1\n",
      "336171 1\n",
      "670535 2\n",
      "873324 1\n",
      "491551 1\n",
      "227857 1\n",
      "512848 1\n",
      "1076061 1\n",
      "446559 1\n",
      "143888 1\n",
      "842453 1\n",
      "1077879 1\n",
      "695506 1\n",
      "479157 1\n",
      "1080371 1\n",
      "545580 1\n",
      "283717 1\n",
      "1119229 1\n",
      "865464 1\n",
      "1022624 1\n",
      "1121705 1\n",
      "542846 1\n",
      "449117 1\n",
      "500488 1\n",
      "783922 1\n",
      "1125238 1\n",
      "1124661 1\n",
      "1118428 1\n",
      "142709 1\n",
      "925300 1\n",
      "779657 1\n",
      "1112565 1\n",
      "833247 1\n",
      "449299 1\n",
      "1023412 1\n",
      "131480 1\n",
      "1113320 1\n",
      "0.0035805555555555547\n"
     ]
    }
   ],
   "source": [
    "#doc2 vec evaluation\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, LabeledSentence\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "papers = pd.read_csv('data-kw-216432.csv')\n",
    "\n",
    "random.seed(1)\n",
    "#select 2000 documens randomly\n",
    "ids_sel = random.sample(list(papers['INDEX']), 2000)\n",
    "\n",
    "def get_abstracts_indices():\n",
    "    #change the dir\n",
    "    #papers = pd.read_csv( 'data-kw-216432.csv' )\n",
    "    # TODO: Get from whole dataset (too)\n",
    "    for i, abstract in enumerate( papers['ABSTRACT'] ):\n",
    "        index = papers['INDEX'][i]\n",
    "        if isinstance( abstract, float ):\n",
    "            #print( 'NO ABSTRACT FOUND at Index = '+ index )\n",
    "            continue\n",
    "        ab_words = abstract.lower().split()  # TODO: CHANGE TO SOMETHING ACTUALLY MEANINGFUL\n",
    "        yield TaggedDocument( words=ab_words, tags=[int(index)] )\n",
    "        #yield LabeledSentence( words=ab_words, tags=[int(index)] )\n",
    "        # TODO: Consider having the list of references as multiple tags/labels?\n",
    "\n",
    "tdocs = list( get_abstracts_indices() )\n",
    "model = Doc2Vec(tdocs, size=500, window=15, min_count=4, workers=4 )        \n",
    "        \n",
    "#define function to compare citation\n",
    "def compare_citations(ID):        \n",
    "    #TODO: evaluation of this   \n",
    "    #ars: take the id as integer\n",
    "    #returns the number of citations in common between sugested and actual\n",
    "    citations_fromdb = list(papers[papers['INDEX'] == ID]['CITATIONS'])\n",
    "    pred_citations =[]\n",
    "    for pair in model.docvecs.most_similar(ID):\n",
    "        ID2, score = pair \n",
    "        pred_citations.append(ID2)\n",
    "    for i in citations_fromdb:\n",
    "        actual_citations = i.split(\";\")\n",
    "    actual_citations = list(map(int, actual_citations))\n",
    "    if len(set(pred_citations) & set(actual_citations))>0:\n",
    "        print(ID,len(set(pred_citations) & set(actual_citations)))\n",
    "    acc = len(set(pred_citations) & set(actual_citations))/(min(len(pred_citations),len(actual_citations)))\n",
    "    return acc\n",
    "    \n",
    "# calculates the accuracy.    \n",
    "accuracy = {}    \n",
    "for ns in ids_sel:\n",
    "    accuracy [ns] = compare_citations(int(ns))\n",
    "print(sum(accuracy[ns] for ns in accuracy) / len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = pd.read_csv('data-kw copy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Final evaluation separating testing and traingin\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "#print \"data dimension\", ts\n",
    "#print \"product attributes \\n\", train.columns.values \n",
    "\n",
    "#file = pd.read_csv('data-kw copy.csv')\n",
    "df = pd.DataFrame(file)\n",
    "# Randomly sample 70% of your dataframe\n",
    "\n",
    "df_test = df.sample(frac=0.1)\n",
    "df_rest = df.loc[~df.index.isin(df_test.index)]\n",
    "\n",
    "df_test.to_csv('data-kw-test.csv',header=False)\n",
    "df_rest.to_csv('data-kw.csv',header=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26596"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
